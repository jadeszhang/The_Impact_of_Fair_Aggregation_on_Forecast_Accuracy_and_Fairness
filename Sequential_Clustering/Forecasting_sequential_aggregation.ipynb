{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# install packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from statsmodels.tsa.api import SimpleExpSmoothing, ExponentialSmoothing, Holt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation(df):\n",
    "    # third_level = [x for x in df.columns if x. == 2]\n",
    "    # second_level = [x for x in df.columns if x.count('_') == 1]\n",
    "    # first_level = [x for x in df.columns if (x.count('_') == 0 and x!= \"total\")]\n",
    "    if ('total' in df.columns):\n",
    "        df = df.drop('total', axis = 1)\n",
    "    perm_lst = []\n",
    "    column_name_lst = []\n",
    "    ele = df.columns[1]\n",
    "    if ele.count('_') == 2:\n",
    "        for x in range(0, len(df.columns)):\n",
    "            # break the string\n",
    "            race_x = df.columns[x].split('_')[0]\n",
    "            gender_x = df.columns[x].split(\"_\")[1]\n",
    "            generation_x = df.columns[x].split(\"_\")[2]\n",
    "        # find the element that doesnt belong to the same group (only one character is diff with others)\n",
    "            for y in range(x+1, len(df.columns)):\n",
    "                if df.columns[y]!=df.columns[x] :\n",
    "                    race_y = df.columns[y].split('_')[0]\n",
    "                    gender_y = df.columns[y].split(\"_\")[1]\n",
    "                    generation_y = df.columns[y].split(\"_\")[2]\n",
    "                    if( (race_x == race_y and gender_x  == gender_y and generation_x != generation_y)\n",
    "                    or (race_x == race_y and gender_x  != gender_y and generation_x == generation_y)\n",
    "                    or(race_x != race_y and gender_x  == gender_y and generation_x == generation_y)):\n",
    "                    \n",
    "                        perm_lst.append([df.columns[x],df.columns[y]])\n",
    "                        column_name_lst.append(df.columns[x]+\" \"+df.columns[y])\n",
    "                    \n",
    "    elif df.columns[0].count('_') == 1:\n",
    "        for x in range(0,len(df.columns)):\n",
    "            # break the string\n",
    "            race_x = df.columns[x].split('_')[0]\n",
    "            gender_x = df.columns[x].split(\"_\")[1]\n",
    "            # find the element that doesnt belong to the same group (only one character is diff with others)\n",
    "            for y in range(x+1, len(df.columns)):\n",
    "                if y!=x :\n",
    "                    race_y = df.columns[y].split('_')[0]\n",
    "                    gender_y = df.columns[y].split(\"_\")[1]\n",
    "                    if( (race_x == race_y and gender_x  != gender_y)\n",
    "                    or (race_x != race_y and gender_x  == gender_y)):\n",
    "                        perm_lst.append([df.columns[x],df.columns[y]])\n",
    "                        column_name_lst.append(df.columns[x]+\" \"+df.columns[y])\n",
    "                        \n",
    "    elif df.columns[0].count('_') == 0:\n",
    "        for x in range(0,len(df.columns)):\n",
    "            for y in range(x+1,len(df.columns)):\n",
    "                if df.columns[y] != df.columns[x] and df.columns[y]!= 'total' and df.columns[x]!='total':\n",
    "                    perm_lst.append([df.columns[x],df.columns[y]])\n",
    "                    column_name_lst.append(df.columns[x]+\" \"+df.columns[y])\n",
    "                    \n",
    "    return perm_lst, column_name_lst\n",
    "\n",
    "\n",
    "\n",
    "####################################### Accuracy\n",
    "\n",
    "def RMSE (df_actual, df_pred):\n",
    "    # create a dataframe to store all the RMSE\n",
    "    df_rmse = pd.DataFrame(columns= df_pred.columns, index=range(1))\n",
    "    # total rmse for all features \n",
    "    total_rmse = 0\n",
    "    # get rmse for each col\n",
    "    for col in df_pred.columns:\n",
    "        rmse = sqrt(mean_squared_error(df_actual[col], df_pred[col]))\n",
    "        df_rmse[col].iloc[0] = rmse\n",
    "        total_rmse = total_rmse + rmse\n",
    "    avg_rmse =  total_rmse/len(df_pred.columns)\n",
    "    num_of_cols = len(df_pred.columns)\n",
    "    return df_rmse, total_rmse, avg_rmse, num_of_cols\n",
    "\n",
    "def MASE(df_actual, df_pred, df_naive):\n",
    "    # create a dataframe to store all the MASE\n",
    "    df_mase = pd.DataFrame(columns= df_pred.columns, index=range(1))\n",
    "    # total rmse for all features \n",
    "    total_mase = 0\n",
    "    # get rmse for each col\n",
    "    for col in df_pred.columns:\n",
    "        # get the MAE_naive\n",
    "        mae_naive = 0\n",
    "        for i in range(0, len(df_naive[col])-1):\n",
    "            abs_diff = abs(df_naive[col].iloc[i+1]-df_naive[col].iloc[i])\n",
    "            mae_naive = mae_naive + abs_diff\n",
    "    \n",
    "        mae_naive = mae_naive/(len(df_naive[col])-1)\n",
    "        \n",
    "        mae_pred = mean_absolute_error(df_actual[col], df_pred[col])\n",
    "        mase = mae_pred/mae_naive\n",
    "        df_mase[col].iloc[0] = mase\n",
    "        total_mase = total_mase + mase\n",
    "        \n",
    "        avg_mase =  total_mase/len(df_pred.columns)\n",
    "        num_of_cols = len(df_pred.columns)\n",
    "    return df_mase, total_mase, avg_mase, num_of_cols\n",
    "\n",
    "\n",
    "\n",
    "####################################### Fairness\n",
    "\n",
    "def total_fairness(df_actual, df_pred, thred, level):\n",
    " \n",
    "    if level == \"l1\":\n",
    "        diffs = []\n",
    "        count_actual = 0\n",
    "    \n",
    "        actual_total = df_actual['total'].iloc[0]\n",
    "        columns = [x for x in df_actual.columns if (x.count(\"_\") == 0) and x != 'total']\n",
    "        perm_lst, column_name_lst = permutation(df_actual[columns])\n",
    "        for x in perm_lst:\n",
    "            if (abs(df_actual[x[0]].iloc[0]/actual_total - df_actual[x[1]].iloc[0]/actual_total)) <= thred:\n",
    "                count_actual = count_actual + 1\n",
    "        totalfair_actual = count_actual/36\n",
    "\n",
    "        count_pred = 0\n",
    "        totalfair_pred = 0\n",
    "        pred_total = df_pred['total'].iloc[0]\n",
    "        columns = [x for x in df_pred.columns if (x.count(\"_\") == 0) and x != 'total']\n",
    "        perm_lst, column_name_lst = permutation(df_pred[columns])\n",
    "        for x in perm_lst:\n",
    "            diff = abs(df_pred[x[0]].iloc[0]/pred_total - df_pred[x[1]].iloc[0]/pred_total)\n",
    "            diffs.append(diff)\n",
    "            if (diff) <= thred:\n",
    "                count_pred = count_pred + 1\n",
    "        totalfair_pred = count_pred/36\n",
    "        return totalfair_actual, totalfair_pred, diffs\n",
    "    \n",
    "    elif level == \"l2\":\n",
    "        diffs = []\n",
    "\n",
    "        count_actual = 0\n",
    "        totalfair_actual = 0\n",
    "        race_columns = [x for x in df_actual.columns if (x.count(\"_\") == 0) and x != 'total']\n",
    "        perm_lst, column_name_lst = permutation(df_actual[race_columns])\n",
    "        for x in perm_lst:\n",
    "            fem1 = x[0] + '_'+'fem'\n",
    "            fem2 = x[1] + \"_\"+'fem'\n",
    "            diff = abs((df_actual[fem1].iloc[0]/df_actual[x[0]].iloc[0]) - (df_actual[fem2].iloc[0]/df_actual[x[1]].iloc[0]))\n",
    "            diffs.append(diff)\n",
    "            if (diff <= thred):\n",
    "                count_actual = count_actual + 1\n",
    "        totalfair_actual = count_actual/36\n",
    "        \n",
    "        count_pred = 0\n",
    "        totalfair_pred = 0\n",
    "        race_columns = [x for x in df_pred.columns if (x.count(\"_\") == 0) and x != 'total']\n",
    "        perm_lst, column_name_lst = permutation(df_pred[race_columns])\n",
    "        for x in perm_lst:\n",
    "            fem1 = x[0] + '_'+'fem'\n",
    "            fem2 = x[1] + \"_\"+'fem'\n",
    "            if (abs((df_pred[fem1].iloc[0]/df_pred[x[0]].iloc[0]) - (df_pred[fem2].iloc[0]/df_pred[x[1]].iloc[0])) <= thred):\n",
    "                count_pred = count_pred + 1\n",
    "        totalfair_pred = count_pred/36\n",
    "        return totalfair_actual, totalfair_pred, diffs\n",
    "    \n",
    "    elif (level == 'l3'):\n",
    "        diffs = []\n",
    "        # constrauct a new tree for race & first-gen-status\n",
    "        race_columns = [x for x in df_actual.columns if (x.count(\"_\") == 0) and x != 'total'] \n",
    "        for race in race_columns:\n",
    "            # r_first = r_male_first + r_fem_first\n",
    "            col_name_firstgen = race + '_' + 'first'\n",
    "            col_male_firstgen = race + '_male_first' \n",
    "            col_fem_firstgen = race + '_fem_first' \n",
    "            df_actual[col_name_firstgen] = df_actual[col_male_firstgen] + df_actual[col_fem_firstgen]\n",
    "            df_pred[col_name_firstgen] = df_pred[col_male_firstgen] + df_pred[col_fem_firstgen]\n",
    "            # r_nFirst = r_male_nFirst + r_fem_nFirst\n",
    "            col_name_nfirstgen = race + '_' + 'nFirst'\n",
    "            col_male_nfirstgen = race + '_male_nFirst' \n",
    "            col_fem_nfirstgen = race + '_fem_nFirst' \n",
    "            df_actual[col_name_nfirstgen] = df_actual[col_male_nfirstgen] + df_actual[col_fem_nfirstgen]\n",
    "            df_pred[col_name_nfirstgen] = df_pred[col_male_nfirstgen] + df_pred[col_fem_nfirstgen]\n",
    "\n",
    "        ################ to calculate the total fairness ##################\n",
    "        perm_lst, column_name_lst = permutation(df_actual[race_columns])\n",
    "        totalfairs_actual = 0\n",
    "        y1 = 0\n",
    "        y2 = 0\n",
    "        y12 = 0\n",
    "        for x in perm_lst:\n",
    "            # y1 = r1_fem/r1 - r2_fem/r2\n",
    "            # y2 = r1_firstgen/r1 - r2_firstgen/r2\n",
    "            fem1 = x[0] + '_'+'fem'\n",
    "            fem2 = x[1] + \"_\"+'fem'\n",
    "            fg1 = x[0] + '_'+'first'\n",
    "            fg2 = x[1] + \"_\"+'first'\n",
    "            diff1 = abs((df_actual[fem1].iloc[0]/df_actual[x[0]].iloc[0]) - (df_actual[fem2].iloc[0]/df_actual[x[1]].iloc[0]))\n",
    "            diffs.append(diff1)\n",
    "\n",
    "            if (diff1 <= thred):\n",
    "                y1 = 1\n",
    "            diff2 = abs((df_actual[fg1].iloc[0]/df_actual[x[0]].iloc[0]) - (df_actual[fg2].iloc[0]/df_actual[x[1]].iloc[0]))\n",
    "            diffs.append(diff2)\n",
    "\n",
    "            if ( diff2 <= thred):\n",
    "                y2 = 1\n",
    "                \n",
    "            y12 = y1 * y2\n",
    "            f = 1/3 * y1 + 1/3 * y2 + 1/3 * y12\n",
    "            totalfairs_actual = totalfairs_actual + f\n",
    "        \n",
    "        totalfair_actual = totalfairs_actual/36\n",
    "        \n",
    "        ####### for prediction\n",
    "        totalfairs_pred = 0\n",
    "        y1 = 0\n",
    "        y2 = 0\n",
    "        y12 = 0\n",
    "        for x in perm_lst:\n",
    "            fem1 = x[0] + '_'+'fem'\n",
    "            fem2 = x[1] + \"_\"+'fem'\n",
    "            fg1 = x[0] + '_'+'first'\n",
    "            fg2 = x[1] + \"_\"+'first'\n",
    "            if (abs((df_pred[fem1].iloc[0]/df_pred[x[0]].iloc[0]) - (df_pred[fem2].iloc[0]/df_pred[x[1]].iloc[0])) <= thred):\n",
    "                y1 =  1\n",
    "            if (abs((df_pred[fg1].iloc[0]/df_pred[x[0]].iloc[0]) - (df_pred[fg2].iloc[0]/df_pred[x[1]].iloc[0])) <= thred):\n",
    "                y2 = 1\n",
    "            y12 = y1 * y2\n",
    "            f = 1/3 * y1 + 1/3 * y2 + 1/3 * y12\n",
    "            totalfairs_pred = totalfairs_pred + f\n",
    "        \n",
    "        totalfair_pred = totalfairs_pred/36\n",
    "        \n",
    "        return totalfair_actual, totalfair_pred, diffs\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.getcwd() + '\\\\' + 'data'\n",
    "files = os.listdir(directory)\n",
    "dfs = {}\n",
    "dfs_standard = {}\n",
    "dfs_sequential = {}\n",
    "for filename in files: # read all the files\n",
    "    if filename.endswith('.csv'):\n",
    "        #concast the file name\n",
    "        dir_filename = \"data/\"+filename\n",
    "        f = pd.read_csv(dir_filename, sep=\"\\t\" , encoding=\"utf-16-le\",  thousands=',')        \n",
    "        dfs[filename] = f\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data cleaning for standard aggreagtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in dfs:\n",
    "\n",
    "    # get the features of df\n",
    "    df_name = name\n",
    "    df_name = df_name.split('.')[0].split('_')\n",
    "    Student_level = df_name[0]\n",
    "    First_gen_status = df_name[1]\n",
    "    Enrollment_status = df_name[2]\n",
    "    Gender = df_name[3]\n",
    "    Residency = df_name[4]\n",
    "    Entry_status = df_name[5]\n",
    "    \n",
    "    \n",
    "    # operation for datasets\n",
    "    df = dfs[name]\n",
    "    df = df.drop(df.index[-1])\n",
    "    df.loc[df['Category'] == 'Chinese', 'Broad category'] = 'East Asain'\n",
    "    df.loc[df['Category'] == 'Taiwanese', 'Broad category'] = 'East Asain'\n",
    "    df.loc[df['Category'] == 'Japanese', 'Broad category'] = 'East Asain'\n",
    "    df.loc[df['Category'] == 'Korean', 'Broad category'] = 'East Asain'\n",
    "    df.loc[df['Category'] == 'White/Middle Eastern', 'Broad category'] = 'White/Middle Eastern'\n",
    "    data_2010_columns = [\"Broad category\", '2010', '2011', '2012', '2013','2014', '2015','2016', '2017', '2018', '2019']\n",
    "    df = df[data_2010_columns]\n",
    "    df = df.groupby(\"Broad category\").sum()\n",
    "    df = df.rename(index = {'American Indian/Alaska Native':'Native', 'African American and Black' : 'Black', 'Hispanic/Latinx': 'Hiapanic', \n",
    "                           'Native Hawaiian and Pacific Islander': 'Islander', 'Southwest Asian/North African' : 'SANA', 'White/Middle Eastern':'WE'})\n",
    "\n",
    "    # transpose\n",
    "    df = df.T\n",
    "    df = df.rename_axis(None, axis = 1).rename_axis('year', axis = 0)\n",
    "    df = df.reset_index()\n",
    "    # store the dataframes into df_standard    \n",
    "\n",
    "    # melt the dataset\n",
    "    columns = df.columns # get all the columns' names\n",
    "    columns = columns[1:] # get the columns names except for Year\n",
    "\n",
    "    df = pd.melt(df, id_vars=[\"year\"], value_vars=columns)\n",
    "    df = df.rename(columns = {0:'Race', 'value':'count'})\n",
    "\n",
    "    # add columns for other features\n",
    "    df[\"Student_Level\"] = Student_level\n",
    "    df[\"First_gen_status\"] = First_gen_status\n",
    "    df[\"Enrollment_status\"] = Enrollment_status\n",
    "    df[\"Gender\"] = Gender\n",
    "    df[\"Residency\"] = Residency\n",
    "    df[\"Entry_status\"] = Entry_status\n",
    "    \n",
    "    dfs_standard[name] = df\n",
    "\n",
    "##################### merge the datasets ########################################\n",
    "\n",
    "names = list(dfs_standard.keys())\n",
    "datasets = list(dfs_standard.values())\n",
    "# concat all the datasets together\n",
    "df_standard = pd.concat(datasets, sort=False, ignore_index = True)\n",
    "df_standard = df_standard.sort_values(by = ['year','Student_Level','Residency','Enrollment_status','Gender'])\n",
    "df_standard = df_standard.reset_index().drop(['index'], axis = 1)\n",
    "df_standard = df_standard.rename(columns = {'variable':'race'})\n",
    "df_standard.columns= df_standard.columns.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data cleaning for sequential aggreagtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.getcwd() + '\\\\' + 'data'\n",
    "files = os.listdir(directory)\n",
    "dfs = {}\n",
    "dfs_standard = {}\n",
    "dfs_sequential = {}\n",
    "for filename in files: # read all the files\n",
    "    if filename.endswith('.csv'):\n",
    "        #concast the file name\n",
    "        dir_filename = \"data/\"+filename\n",
    "        f = pd.read_csv(dir_filename,  sep=\"\\t\" , encoding=\"utf-16-le\",  thousands=',')        \n",
    "        dfs[filename] = f\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20\n"
     ]
    }
   ],
   "source": [
    "# read the file of sequential aggreation\n",
    "# for alpha in ['0', '0.2', '0.4','0.6', '0.8', '1']:\n",
    "#     for epsilon in ['0.05', '0.1', '0.15','0.02', '0.25', '0.03']:\n",
    "\n",
    "# a 2-D list to store all the categories\n",
    "rules = []\n",
    "epsilon = 0\n",
    "\n",
    "for alpha in ['0.8']:\n",
    "    for epsilon in ['0.20']:\n",
    "        rule_name = alpha + \" \" + epsilon +\".txt\"\n",
    "print(epsilon)\n",
    "\n",
    "file_path = os.getcwd() + '\\\\' + 'sequential aggregation rules' + '\\\\' + rule_name\n",
    "\n",
    "# open the file\n",
    "rule_file = open(file_path, 'r')\n",
    "# read every line \n",
    "lines = rule_file.readlines()\n",
    "for line in lines:\n",
    "    if (line !='\\n'):\n",
    "        line = line.rstrip('\\n').split(',')\n",
    "        rules.append(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define labels\n",
    "def label_race (row):\n",
    "    if row['Category'] in rules[0]:\n",
    "        return 'G1'\n",
    "    \n",
    "    if row['Category'] in rules[1]:\n",
    "        return 'G2'\n",
    "    \n",
    "    if row['Category'] in rules[2] :\n",
    "        return 'G3'\n",
    "    \n",
    "    if row['Category'] in rules[3] :\n",
    "        return 'G4'\n",
    "    \n",
    "    if row['Category'] in rules[4] :\n",
    "        return 'G5'\n",
    "    \n",
    "    if row['Category'] in rules[5]:\n",
    "        return 'G6'\n",
    "    \n",
    "    if row['Category'] in rules[6] :\n",
    "        return 'G7'\n",
    "        \n",
    "    if row['Category'] in rules[7] :\n",
    "        return 'G8'    \n",
    "    \n",
    "    if row['Category'] in rules[8] :\n",
    "        return 'G9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in dfs:\n",
    "\n",
    "    # get the features of df\n",
    "    df_name = name\n",
    "    df_name = df_name.split('.')[0].split('_')\n",
    "    Student_level = df_name[0]\n",
    "    First_gen_status = df_name[1]\n",
    "    Enrollment_status = df_name[2]\n",
    "    Gender = df_name[3]\n",
    "    Residency = df_name[4]\n",
    "    Entry_status = df_name[5]\n",
    "    \n",
    "    \n",
    "    # operation for datasets\n",
    "    df = dfs[name]\n",
    "    # # apply the sequential aggreagtion rules\n",
    "    df['SA category'] = df.apply(lambda row: label_race(row), axis=1)\n",
    "    data_2010_columns = [\"SA category\", '2010', '2011', '2012', '2013','2014', '2015','2016', '2017', '2018', '2019']\n",
    "    df = df[data_2010_columns]\n",
    "    df = df.groupby(\"SA category\").sum()\n",
    "#     # transpose\n",
    "    df = df.T\n",
    "    df = df.rename_axis(None, axis = 1).rename_axis('year', axis = 0)\n",
    "    df = df.reset_index()\n",
    "    # store the dataframes into df_standard    \n",
    "\n",
    "    # melt the dataset\n",
    "    columns = df.columns # get all the columns' names\n",
    "    columns = columns[1:] # get the columns names except for Year\n",
    "\n",
    "    df = pd.melt(df, id_vars=[\"year\"], value_vars=columns)\n",
    "    df = df.rename(columns = {0:'Race', 'value':'count'})\n",
    "\n",
    "    # add columns for other features\n",
    "    df[\"Student_Level\"] = Student_level\n",
    "    df[\"First_gen_status\"] = First_gen_status\n",
    "    df[\"Enrollment_status\"] = Enrollment_status\n",
    "    df[\"Gender\"] = Gender\n",
    "    df[\"Residency\"] = Residency\n",
    "    df[\"Entry_status\"] = Entry_status\n",
    "    \n",
    "    dfs_sequential[name] = df\n",
    "\n",
    "##################### merge the datasets ########################################\n",
    "\n",
    "names = list(dfs_sequential.keys())\n",
    "datasets = list(dfs_sequential.values())\n",
    "# concat all the datasets together\n",
    "df_sequential = pd.concat(datasets, sort=False, ignore_index = True)\n",
    "df_sequential = df_sequential.sort_values(by = ['year','Student_Level','Residency','Enrollment_status','Gender'])\n",
    "df_sequential = df_sequential.reset_index().drop(['index'], axis = 1)\n",
    "df_sequential = df_sequential.rename(columns = {'variable':'race'})\n",
    "df_sequential.columns= df_sequential.columns.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "# to perform the prediction task\n",
    "# parameters: input df\n",
    "# return: total rmse and the predicted result (one row)\n",
    "#####\n",
    "def pred(df, period):\n",
    "    pred_results = []\n",
    "    df_vars = df.columns \n",
    "    rmse = []\n",
    "    for cl in df_vars:\n",
    "        # fit the model for every column \n",
    "        # size of training set: 21, size of test set: 1\n",
    "        \n",
    "        train = df[cl].iloc[0:-period]\n",
    "        test = df[cl].iloc[-period:]\n",
    "        # fit the model\n",
    "        # model = auto_arima(train)\n",
    "        model = SimpleExpSmoothing(train)\n",
    "        fit = model.fit()\n",
    "        pred = fit.forecast(period)\n",
    "        # pred = model.predict(n_periods=period)\n",
    "        # convert the pred series to dataframe\n",
    "        df_pred = pd.DataFrame(pred)\n",
    "        df_pred = df_pred.rename({0:cl}, axis = 1)\n",
    "        # attach the pred to the list\n",
    "        pred_results.append(df_pred)\n",
    "        # calculate RMSE\n",
    "        RMSE = sqrt(mean_squared_error(test, pred))\n",
    "        rmse.append(RMSE)\n",
    "    pred = pd.concat(pred_results, axis=1)\n",
    "    total_rmse = sum(rmse)/len(rmse)\n",
    "    \n",
    "    return (pred, total_rmse)\n",
    "\n",
    "def get_proportion_AHP(df, feature_name):\n",
    "\n",
    "    # find the forecast results for residency\n",
    "\n",
    "    ### calculate the porpotion of each residency w.r.t total amount\n",
    "    # get categories of residency\n",
    "    feature_lst = list(df[feature_name].unique())\n",
    "    # list to stor mean proportions for every subcategory in the feature\n",
    "    mean_ppt_feature_lst = []\n",
    "    # dataframe for different features\n",
    "    df_new = df.pivot_table(index= \"year\", columns = [feature_name], values = \"count\", aggfunc= \"sum\")\n",
    "    # create a sum column \n",
    "    df_new['total'] = df_new.sum(axis = 1)\n",
    "    # get proportion for every race in total trips\n",
    "    for i in feature_lst:\n",
    "        sum_ppt = 0\n",
    "        # for every year\n",
    "        for j in range(0, len(df_new)):\n",
    "            # proportion of the race (race/total) in every year\n",
    "            ppt = df_new[i].iloc[j]/df_new['total'].iloc[j]\n",
    "            # add all proportions\n",
    "            sum_ppt = sum_ppt + ppt\n",
    "        # get the mean of the sum\n",
    "        mean_ppt = sum_ppt/len(df_new)\n",
    "        mean_ppt_feature_lst.append(mean_ppt)\n",
    "    feature_ppt = dict(zip(feature_lst, mean_ppt_feature_lst))\n",
    "\n",
    "    return feature_ppt\n",
    "\n",
    "def get_proportion_PHA (df, feature_name):\n",
    "\n",
    "    # find the forecast results for residency\n",
    "\n",
    "    ### calculate the porpotion of each residency w.r.t total amount\n",
    "    # get categories of residency\n",
    "    feature_lst = list(df[feature_name].unique())\n",
    "    # list to stor mean proportions for every race\n",
    "    mean_ppt_feature_lst = []\n",
    "    # dataframe for races\n",
    "    df_new = df.pivot_table(index= \"year\", columns = [feature_name], values = \"count\", aggfunc= \"sum\")\n",
    "    df_new['total'] = df_new.sum(axis = 1)\n",
    "    # get proportion for every race in total trips\n",
    "    for i in feature_lst:\n",
    "        mean_ppt = df_new[i].mean()/df_new['total'].mean()\n",
    "        mean_ppt_feature_lst.append(mean_ppt)\n",
    "    feature_ppt = dict(zip(feature_lst, mean_ppt_feature_lst))\n",
    "\n",
    "    return feature_ppt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "standard aggreagtion & sequential aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecasts (df, proportion):\n",
    "    df.columns= df.columns.str.lower()\n",
    "    # aggregate enrollment_status and entry_status and residency together\n",
    "    ###################################################################\n",
    "\n",
    "    ## to seperate it into 3 levels\n",
    "    df_std_l3 = df.pivot_table(index= \"year\", columns = [\"race\", \"gender\", \"first_gen_status\"], values = \"count\", aggfunc= \"sum\")\n",
    "    df_std_l3.columns = ['_'.join(str(s).strip() for s in col if s) for col in df_std_l3.columns]\n",
    "\n",
    "    ## 2 levels: total -> race -> gender\n",
    "    df_std_l2 = df.pivot_table(index= \"year\", columns = [\"race\", \"gender\"], values = \"count\", aggfunc= \"sum\")\n",
    "    df_std_l2.columns = ['_'.join(str(s).strip() for s in col if s) for col in df_std_l2.columns]\n",
    "\n",
    "    ## 1 level: total -> first_gen_status\n",
    "    df_std_l1 = df.pivot_table(index= \"year\", columns = [\"race\"], values = \"count\", aggfunc= \"sum\")\n",
    "\n",
    "    ## total\n",
    "    df_total = df.pivot_table(index= \"year\", values = \"count\", aggfunc= \"sum\")\n",
    "    df_total = df_total.rename(columns={\"count\": \"total\"})\n",
    "\n",
    "    # join the dateframes\n",
    "    df_std_full_info = df_total.join(df_std_l1).join(df_std_l2).join(df_std_l3)\n",
    "\n",
    "    # change the index year to day-time format\n",
    "    df_std_full_info.index = pd.to_datetime(df_std_full_info.index, format='%Y')\n",
    "    df_std_full_info = df_std_full_info.to_period(\"Y\")\n",
    "    df_std_full_info = df_std_full_info.to_timestamp()\n",
    "    df_std_full_info\n",
    "    df_std_full_info.head(10)\n",
    "\n",
    "    ################################### prediction for data without info loss #########################\n",
    "    pred_df_std_full_info, pred_df_std_full_info_rmse = pred(df_std_full_info, 1)\n",
    "    pred_df_std_full_info = pd.concat([df_std_full_info,pred_df_std_full_info])\n",
    "\n",
    "    ################################### prediction for data with info loss at level 3 #########################\n",
    "    df_std_loss_l3 = df_total.join(df_std_l1).join(df_std_l2)\n",
    "    df_std_loss_l3.index = pd.to_datetime(df_std_loss_l3.index, format='%Y')\n",
    "    df_std_loss_l3 = df_std_loss_l3.to_period(\"Y\")\n",
    "    df_std_loss_l3 = df_std_loss_l3.to_timestamp()\n",
    "    pred_df_std_loss_l3, pred_df_std_loss_l3_rmse = pred(df_std_loss_l3,1)\n",
    "\n",
    "    # create the forecast results for every residency using AHP\n",
    "    if (proportion == 'AHP'):\n",
    "        fg_ppt_AHP = get_proportion_AHP(df, \"first_gen_status\")\n",
    "    elif (proportion == 'PHA'):\n",
    "        fg_ppt_AHP = get_proportion_PHA(df, \"first_gen_status\")\n",
    "    # create the forecast results for every residency using AHP\n",
    "    for col in pred_df_std_loss_l3:\n",
    "        if col.count('_') == 1:\n",
    "            gender = col.split(\"_\")[-1]\n",
    "            # find all the first_gen_status for the state\n",
    "            for fg in list(df['first_gen_status'].unique()):\n",
    "                new_col_name = col + \"_\" + fg\n",
    "                pred_df_std_loss_l3[new_col_name] = pred_df_std_loss_l3[col]*fg_ppt_AHP[fg]\n",
    "    pred_df_std_loss_l3 = pd.concat([df_std_full_info,pred_df_std_loss_l3])\n",
    "    \n",
    "    ################################### prediction for data with info loss at level 2 #########################\n",
    "\n",
    "    # join the dateframes without race and residency\n",
    "    df_std_loss_l2 = df_total.join(df_std_l1)\n",
    "    df_std_loss_l2.index = pd.to_datetime(df_std_loss_l2.index, format='%Y')\n",
    "    df_std_loss_l2 = df_std_loss_l2.to_period(\"Y\")\n",
    "    df_std_loss_l2 = df_std_loss_l2.to_timestamp()\n",
    "    pred_df_std_loss_l2, pred_df_std_loss_l2_rmse = pred(df_std_loss_l2,1)\n",
    "    \n",
    "    if (proportion == 'AHP'):\n",
    "        gender_ppt = get_proportion_AHP(df, \"gender\")\n",
    "    elif (proportion == 'PHA'):\n",
    "        gender_ppt = get_proportion_PHA(df, \"gender\")\n",
    "        \n",
    "    # create the forecast results for every gender using AHP\n",
    "    for col in pred_df_std_loss_l2:\n",
    "        if col != 'total':\n",
    "            race = col\n",
    "            # find all the genders for the race\n",
    "            for gen in list(df['gender'].unique()):\n",
    "                new_col_name = col + \"_\" + gen\n",
    "                pred_df_std_loss_l2[new_col_name] = pred_df_std_loss_l2[col]*gender_ppt[gen]\n",
    "\n",
    "    if (proportion == 'AHP'):\n",
    "        fg_ppt = get_proportion_AHP(df, \"first_gen_status\")\n",
    "    elif (proportion == 'PHA'):\n",
    "        fg_ppt = get_proportion_PHA(df, \"first_gen_status\")\n",
    "        \n",
    "    for col in pred_df_std_loss_l2:\n",
    "        if col.count('_') == 1:\n",
    "            gender = col.split(\"_\")[-1]\n",
    "            # find all the regions for the state\n",
    "            for fg in list(df['first_gen_status'].unique()):\n",
    "                new_col_name = col + \"_\" + fg\n",
    "                pred_df_std_loss_l2[new_col_name] = pred_df_std_loss_l2[col]*fg_ppt[fg]\n",
    "\n",
    "    # attach the row to the original dataframe as the last row\n",
    "    pred_df_std_loss_l2 = pd.concat([df_std_full_info,pred_df_std_loss_l2])\n",
    "    pred_df_std_loss_l2\n",
    "    # output the csv result\n",
    "    \n",
    "    \n",
    "    # return all the results\n",
    "    return pred_df_std_full_info, pred_df_std_loss_l3, pred_df_std_loss_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### standard aggregation\n",
    "\n",
    "## AHP\n",
    "AHP_pred_df_std_full_info, AHP_pred_df_std_loss_l3, AHP_pred_df_std_loss_l2 = forecasts(df_standard, \"AHP\")\n",
    "PHA_pred_df_std_full_info, PHA_pred_df_std_loss_l3, PHA_pred_df_std_loss_l2 = forecasts(df_standard, \"PHA\")\n",
    "\n",
    "## PHA\n",
    "AHP_pred_df_seq_full_info, AHP_pred_df_seq_loss_l3, AHP_pred_df_seq_loss_l2 = forecasts(df_sequential, \"AHP\")\n",
    "PHA_pred_df_seq_full_info, PHA_pred_df_seq_loss_l3, PHA_pred_df_seq_loss_l2 = forecasts(df_sequential, \"PHA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "      <th>G3</th>\n",
       "      <th>G4</th>\n",
       "      <th>G5</th>\n",
       "      <th>G6</th>\n",
       "      <th>G7</th>\n",
       "      <th>G8</th>\n",
       "      <th>G9</th>\n",
       "      <th>...</th>\n",
       "      <th>G7_male_first</th>\n",
       "      <th>G7_male_nFirst</th>\n",
       "      <th>G8_fem_first</th>\n",
       "      <th>G8_fem_nFirst</th>\n",
       "      <th>G8_male_first</th>\n",
       "      <th>G8_male_nFirst</th>\n",
       "      <th>G9_fem_first</th>\n",
       "      <th>G9_fem_nFirst</th>\n",
       "      <th>G9_male_first</th>\n",
       "      <th>G9_male_nFirst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>30723.0</td>\n",
       "      <td>964.0</td>\n",
       "      <td>4971.0</td>\n",
       "      <td>966.0</td>\n",
       "      <td>17672.0</td>\n",
       "      <td>760.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>3572.000000</td>\n",
       "      <td>148.0</td>\n",
       "      <td>1569.0</td>\n",
       "      <td>...</td>\n",
       "      <td>984.000000</td>\n",
       "      <td>631.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>467.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>442.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>33939.0</td>\n",
       "      <td>1026.0</td>\n",
       "      <td>5472.0</td>\n",
       "      <td>1545.0</td>\n",
       "      <td>18876.0</td>\n",
       "      <td>935.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>3746.000000</td>\n",
       "      <td>213.0</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>...</td>\n",
       "      <td>964.000000</td>\n",
       "      <td>696.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>499.0</td>\n",
       "      <td>593.0</td>\n",
       "      <td>362.0</td>\n",
       "      <td>511.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>36940.0</td>\n",
       "      <td>1126.0</td>\n",
       "      <td>6093.0</td>\n",
       "      <td>1898.0</td>\n",
       "      <td>20375.0</td>\n",
       "      <td>1153.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>3730.000000</td>\n",
       "      <td>246.0</td>\n",
       "      <td>2131.0</td>\n",
       "      <td>...</td>\n",
       "      <td>992.000000</td>\n",
       "      <td>742.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>563.0</td>\n",
       "      <td>641.0</td>\n",
       "      <td>373.0</td>\n",
       "      <td>554.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>39328.0</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>6624.0</td>\n",
       "      <td>2350.0</td>\n",
       "      <td>21357.0</td>\n",
       "      <td>1270.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>3631.000000</td>\n",
       "      <td>294.0</td>\n",
       "      <td>2387.0</td>\n",
       "      <td>...</td>\n",
       "      <td>893.000000</td>\n",
       "      <td>711.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>662.0</td>\n",
       "      <td>742.0</td>\n",
       "      <td>419.0</td>\n",
       "      <td>564.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>41884.0</td>\n",
       "      <td>1281.0</td>\n",
       "      <td>7475.0</td>\n",
       "      <td>2817.0</td>\n",
       "      <td>21450.0</td>\n",
       "      <td>1695.0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>3781.000000</td>\n",
       "      <td>371.0</td>\n",
       "      <td>2729.0</td>\n",
       "      <td>...</td>\n",
       "      <td>907.000000</td>\n",
       "      <td>742.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>785.0</td>\n",
       "      <td>838.0</td>\n",
       "      <td>515.0</td>\n",
       "      <td>591.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>42233.0</td>\n",
       "      <td>1292.0</td>\n",
       "      <td>7849.0</td>\n",
       "      <td>2920.0</td>\n",
       "      <td>21337.0</td>\n",
       "      <td>1749.0</td>\n",
       "      <td>314.0</td>\n",
       "      <td>3556.000000</td>\n",
       "      <td>383.0</td>\n",
       "      <td>2833.0</td>\n",
       "      <td>...</td>\n",
       "      <td>818.000000</td>\n",
       "      <td>700.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>824.0</td>\n",
       "      <td>834.0</td>\n",
       "      <td>515.0</td>\n",
       "      <td>660.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>45007.0</td>\n",
       "      <td>1413.0</td>\n",
       "      <td>9033.0</td>\n",
       "      <td>3257.0</td>\n",
       "      <td>21653.0</td>\n",
       "      <td>1861.0</td>\n",
       "      <td>364.0</td>\n",
       "      <td>3640.000000</td>\n",
       "      <td>433.0</td>\n",
       "      <td>3353.0</td>\n",
       "      <td>...</td>\n",
       "      <td>810.000000</td>\n",
       "      <td>754.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1091.0</td>\n",
       "      <td>923.0</td>\n",
       "      <td>674.0</td>\n",
       "      <td>665.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>45914.0</td>\n",
       "      <td>1447.0</td>\n",
       "      <td>9639.0</td>\n",
       "      <td>3354.0</td>\n",
       "      <td>21723.0</td>\n",
       "      <td>1838.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>3483.000000</td>\n",
       "      <td>462.0</td>\n",
       "      <td>3568.0</td>\n",
       "      <td>...</td>\n",
       "      <td>707.000000</td>\n",
       "      <td>778.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>1252.0</td>\n",
       "      <td>940.0</td>\n",
       "      <td>728.0</td>\n",
       "      <td>648.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>46903.0</td>\n",
       "      <td>1505.0</td>\n",
       "      <td>10098.0</td>\n",
       "      <td>3425.0</td>\n",
       "      <td>21945.0</td>\n",
       "      <td>1806.0</td>\n",
       "      <td>402.0</td>\n",
       "      <td>3624.000000</td>\n",
       "      <td>435.0</td>\n",
       "      <td>3663.0</td>\n",
       "      <td>...</td>\n",
       "      <td>692.000000</td>\n",
       "      <td>854.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1373.0</td>\n",
       "      <td>943.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>635.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>47269.0</td>\n",
       "      <td>1515.0</td>\n",
       "      <td>10552.0</td>\n",
       "      <td>3676.0</td>\n",
       "      <td>21362.0</td>\n",
       "      <td>1862.0</td>\n",
       "      <td>346.0</td>\n",
       "      <td>3678.000000</td>\n",
       "      <td>521.0</td>\n",
       "      <td>3757.0</td>\n",
       "      <td>...</td>\n",
       "      <td>661.000000</td>\n",
       "      <td>876.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>1408.0</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>684.0</td>\n",
       "      <td>658.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>46903.0</td>\n",
       "      <td>1505.0</td>\n",
       "      <td>10098.0</td>\n",
       "      <td>3425.0</td>\n",
       "      <td>21945.0</td>\n",
       "      <td>1806.0</td>\n",
       "      <td>402.0</td>\n",
       "      <td>3640.333365</td>\n",
       "      <td>435.0</td>\n",
       "      <td>3663.0</td>\n",
       "      <td>...</td>\n",
       "      <td>692.798678</td>\n",
       "      <td>854.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1373.0</td>\n",
       "      <td>943.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>635.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows Ã— 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              total      G1       G2      G3       G4      G5     G6  \\\n",
       "2010-01-01  30723.0   964.0   4971.0   966.0  17672.0   760.0  101.0   \n",
       "2011-01-01  33939.0  1026.0   5472.0  1545.0  18876.0   935.0  161.0   \n",
       "2012-01-01  36940.0  1126.0   6093.0  1898.0  20375.0  1153.0  188.0   \n",
       "2013-01-01  39328.0  1185.0   6624.0  2350.0  21357.0  1270.0  230.0   \n",
       "2014-01-01  41884.0  1281.0   7475.0  2817.0  21450.0  1695.0  285.0   \n",
       "2015-01-01  42233.0  1292.0   7849.0  2920.0  21337.0  1749.0  314.0   \n",
       "2016-01-01  45007.0  1413.0   9033.0  3257.0  21653.0  1861.0  364.0   \n",
       "2017-01-01  45914.0  1447.0   9639.0  3354.0  21723.0  1838.0  400.0   \n",
       "2018-01-01  46903.0  1505.0  10098.0  3425.0  21945.0  1806.0  402.0   \n",
       "2019-01-01  47269.0  1515.0  10552.0  3676.0  21362.0  1862.0  346.0   \n",
       "2019-01-01  46903.0  1505.0  10098.0  3425.0  21945.0  1806.0  402.0   \n",
       "\n",
       "                     G7     G8      G9  ...  G7_male_first  G7_male_nFirst  \\\n",
       "2010-01-01  3572.000000  148.0  1569.0  ...     984.000000           631.0   \n",
       "2011-01-01  3746.000000  213.0  1965.0  ...     964.000000           696.0   \n",
       "2012-01-01  3730.000000  246.0  2131.0  ...     992.000000           742.0   \n",
       "2013-01-01  3631.000000  294.0  2387.0  ...     893.000000           711.0   \n",
       "2014-01-01  3781.000000  371.0  2729.0  ...     907.000000           742.0   \n",
       "2015-01-01  3556.000000  383.0  2833.0  ...     818.000000           700.0   \n",
       "2016-01-01  3640.000000  433.0  3353.0  ...     810.000000           754.0   \n",
       "2017-01-01  3483.000000  462.0  3568.0  ...     707.000000           778.0   \n",
       "2018-01-01  3624.000000  435.0  3663.0  ...     692.000000           854.0   \n",
       "2019-01-01  3678.000000  521.0  3757.0  ...     661.000000           876.0   \n",
       "2019-01-01  3640.333365  435.0  3663.0  ...     692.798678           854.0   \n",
       "\n",
       "            G8_fem_first  G8_fem_nFirst  G8_male_first  G8_male_nFirst  \\\n",
       "2010-01-01          62.0           24.0           38.0            24.0   \n",
       "2011-01-01          93.0           39.0           49.0            32.0   \n",
       "2012-01-01         113.0           40.0           52.0            41.0   \n",
       "2013-01-01         109.0           59.0           76.0            50.0   \n",
       "2014-01-01         134.0           84.0           93.0            60.0   \n",
       "2015-01-01         132.0           97.0           80.0            74.0   \n",
       "2016-01-01         142.0          129.0           72.0            90.0   \n",
       "2017-01-01         149.0          135.0           69.0           109.0   \n",
       "2018-01-01         146.0          127.0           72.0            90.0   \n",
       "2019-01-01         159.0          158.0           95.0           109.0   \n",
       "2019-01-01         146.0          127.0           72.0            90.0   \n",
       "\n",
       "            G9_fem_first  G9_fem_nFirst  G9_male_first  G9_male_nFirst  \n",
       "2010-01-01         356.0          467.0          304.0           442.0  \n",
       "2011-01-01         499.0          593.0          362.0           511.0  \n",
       "2012-01-01         563.0          641.0          373.0           554.0  \n",
       "2013-01-01         662.0          742.0          419.0           564.0  \n",
       "2014-01-01         785.0          838.0          515.0           591.0  \n",
       "2015-01-01         824.0          834.0          515.0           660.0  \n",
       "2016-01-01        1091.0          923.0          674.0           665.0  \n",
       "2017-01-01        1252.0          940.0          728.0           648.0  \n",
       "2018-01-01        1373.0          943.0          712.0           635.0  \n",
       "2019-01-01        1408.0         1007.0          684.0           658.0  \n",
       "2019-01-01        1373.0          943.0          712.0           635.0  \n",
       "\n",
       "[11 rows x 64 columns]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AHP_pred_df_seq_full_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_datasets = {\n",
    "    \n",
    "    \n",
    "            'AHP_pred_df_std_full_info':AHP_pred_df_std_full_info,\n",
    "            'AHP_pred_df_seq_full_info': AHP_pred_df_seq_full_info,\n",
    "            'PHA_pred_df_std_full_info':PHA_pred_df_std_full_info,\n",
    "            'PHA_pred_df_seq_full_info':PHA_pred_df_seq_full_info,\n",
    "            \n",
    "            \n",
    "            'AHP_pred_df_std_loss_l3': AHP_pred_df_std_loss_l3,\n",
    "            'PHA_pred_df_std_loss_l3':PHA_pred_df_std_loss_l3,\n",
    "            \n",
    "            'AHP_pred_df_seq_loss_l3': AHP_pred_df_seq_loss_l3,\n",
    "            'PHA_pred_df_seq_loss_l3':PHA_pred_df_seq_loss_l3,\n",
    "            \n",
    "            'AHP_pred_df_std_loss_l2': AHP_pred_df_std_loss_l2,\n",
    "            'PHA_pred_df_std_loss_l2': PHA_pred_df_std_loss_l2,\n",
    "            \n",
    "            'AHP_pred_df_seq_loss_l2': AHP_pred_df_seq_loss_l2,\n",
    "            'PHA_pred_df_seq_loss_l2': PHA_pred_df_seq_loss_l2,\n",
    "            \n",
    "            }\n",
    "\n",
    "################### RMSE ########################\n",
    "\n",
    "# get the rmse for different level...\n",
    "character_datasets_rmse = {}\n",
    "character_datasets_rmse_total = {}\n",
    "for x in character_datasets:\n",
    "    # bottom level\n",
    "    bottom_level_cols = [col for col in character_datasets[x].columns if col.count(\"_\") == 2]\n",
    "    # second level\n",
    "    sec_level_cols = [col for col in character_datasets[x].columns if col.count(\"_\") == 1]\n",
    "    # first level\n",
    "    fst_level_cols = [col for col in character_datasets[x].columns if (col.count(\"_\") == 0 and col != \"total\")]\n",
    "    # total\n",
    "    total = ['total']\n",
    "    levels = [bottom_level_cols,sec_level_cols, fst_level_cols, total]\n",
    "    levels_name = ['bottom_level_cols','sec_level_cols', 'fst_level_cols', 'total']\n",
    "    for i in range(0, len(levels)):\n",
    "        df_actual = character_datasets[x][levels[i]].iloc[-2:-1]\n",
    "        df_pred = character_datasets[x][levels[i]].iloc[-1:]\n",
    "        rmse_df, total_rmse, avg_rmse, num_of_cols = RMSE(df_actual,df_pred)\n",
    "        key = x + \" \" + levels_name[i]\n",
    "        \n",
    "        character_datasets_rmse[key] = [rmse_df, total_rmse, avg_rmse, num_of_cols]\n",
    "        character_datasets_rmse_total[key] = [total_rmse, avg_rmse, num_of_cols]\n",
    "        \n",
    "# get the MASE for different level\n",
    "character_datasets_mase = {}\n",
    "character_datasets_mase_total = {}\n",
    "for x in character_datasets:\n",
    "    # bottom level\n",
    "    bottom_level_cols = [col for col in character_datasets[x].columns if col.count(\"_\") == 2]\n",
    "    # second level\n",
    "    sec_level_cols = [col for col in character_datasets[x].columns if col.count(\"_\") == 1]\n",
    "    # first level\n",
    "    fst_level_cols = [col for col in character_datasets[x].columns if (col.count(\"_\") == 0 and col != \"total\")]\n",
    "    # total\n",
    "    total = ['total']\n",
    "    levels = [bottom_level_cols,sec_level_cols, fst_level_cols, total]\n",
    "    levels_name = ['bottom_level_cols','sec_level_cols', 'fst_level_cols', 'total']\n",
    "    for i in range(0, len(levels)):\n",
    "        df_actual = character_datasets[x][levels[i]].iloc[-2:-1]\n",
    "        df_pred = character_datasets[x][levels[i]].iloc[-1:]\n",
    "        df_naive = character_datasets[x][levels[i]].iloc[:-1]\n",
    "        mase_df, total_mase, avg_mase, num_of_cols = MASE(df_actual,df_pred,df_naive)\n",
    "        key = x + \" \" + levels_name[i]\n",
    "        character_datasets_mase[key] = [mase_df, total_mase, avg_mase, num_of_cols]\n",
    "        character_datasets_mase_total[key] = [total_mase, avg_mase, num_of_cols]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "### calculate total fairness\n",
    "character_datasets_fairness = {}\n",
    "\n",
    "\n",
    "for x in character_datasets:\n",
    "    # bottom level\n",
    "    bottom_level_cols = [col for col in character_datasets[x].columns if col.count(\"_\") == 2]\n",
    "    # second level\n",
    "    sec_level_cols = [col for col in character_datasets[x].columns if col.count(\"_\") == 1]\n",
    "    # first level\n",
    "    fst_level_cols = [col for col in character_datasets[x].columns if (col.count(\"_\") == 0 and col != \"total\")]\n",
    "    # cols\n",
    "    cols = bottom_level_cols + sec_level_cols + fst_level_cols\n",
    "    levels = [\"l3\", 'l2', 'l1']\n",
    "    for i in range(0, len(levels)):\n",
    "        # get the mean of all the time periods \n",
    "        # drop the total \n",
    "        df_actual = pd.DataFrame(character_datasets[x].mean(axis = 0)).T\n",
    "        df_pred = character_datasets[x].iloc[-1:]\n",
    "        \n",
    "        level = levels[i]\n",
    "        epsilon = 0.01\n",
    "        totalfair_actual, totalfair_pred, diff = total_fairness(df_actual,df_pred,float(epsilon),level)\n",
    "        \n",
    "        key = x + \" \" + levels_name[i]\n",
    "        character_datasets_fairness[key] = [totalfair_actual, totalfair_pred]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################## result for standard aggregation\n",
    "# file_path = os.getcwd() + '\\\\' + 'results' + '\\\\' + 'accuracy_standard ' + rule_name\n",
    "# with open(file_path, 'w+') as f:\n",
    "#     print('RMSE', file = f)\n",
    "#     print('All info aval (most accurate)', file = f)\n",
    "#     print('    level 3', file = f)\n",
    "#     print('      ' + str(character_datasets_rmse['AHP_pred_df_std_full_info bottom_level_cols'][2]), file = f)\n",
    "#     print('    level 2', file = f)\n",
    "#     print('      ' + str(character_datasets_rmse['AHP_pred_df_std_full_info sec_level_cols'][2]), file = f)\n",
    "#     print('    level 1', file = f)\n",
    "#     print('      ' + str(character_datasets_rmse['AHP_pred_df_std_full_info fst_level_cols'][2]), file = f)\n",
    "#     print('    total', file = f)\n",
    "#     print('      ' + str(character_datasets_rmse['AHP_pred_df_std_full_info total'][2]), file = f)   \n",
    "#     print()\n",
    "    \n",
    "#     print('MASE', file = f)\n",
    "#     print('All info aval (most accurate)', file = f)\n",
    "#     print('    level 3', file = f)\n",
    "#     print('      ' + str(character_datasets_mase['AHP_pred_df_std_full_info bottom_level_cols'][2]), file = f)\n",
    "#     print('    level 2', file = f)\n",
    "#     print('      ' + str(character_datasets_mase['AHP_pred_df_std_full_info sec_level_cols'][2]), file = f)\n",
    "#     print('    level 1', file = f)\n",
    "#     print('      ' + str(character_datasets_mase['AHP_pred_df_std_full_info fst_level_cols'][2]), file = f)\n",
    "#     print('    total', file = f)\n",
    "#     print('      ' + str(character_datasets_mase['AHP_pred_df_std_full_info total'][2]), file = f)   \n",
    "#     print('\\n', file = f)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     print('RMSE', file = f)\n",
    "#     print('loss info at level 3 (AHP)', file = f)\n",
    "#     print('    level 3', file = f)\n",
    "#     print('      ' + str(character_datasets_rmse['AHP_pred_df_std_loss_l3 bottom_level_cols'][2]), file = f)\n",
    "\n",
    "#     print('RMSE', file = f)\n",
    "#     print('loss info at level 3 (PHA)', file = f)\n",
    "#     print('    level 3', file = f)\n",
    "#     print('      ' + str(character_datasets_rmse['PHA_pred_df_std_loss_l3 bottom_level_cols'][2]), file = f)\n",
    "    \n",
    "#     print('MASE', file = f)\n",
    "#     print('loss info at level 3 (AHP)', file = f)\n",
    "#     print('    level 3', file = f)\n",
    "#     print('      ' + str(character_datasets_mase['AHP_pred_df_std_loss_l3 bottom_level_cols'][2]), file = f)\n",
    "\n",
    "#     print('MASE', file = f)\n",
    "#     print('loss info at level 3 (PHA)', file = f)\n",
    "#     print('    level 3', file = f)\n",
    "#     print('      ' + str(character_datasets_mase['PHA_pred_df_std_loss_l3 bottom_level_cols'][2]), file = f)\n",
    "    \n",
    "#     print('\\n', file = f)\n",
    "\n",
    "    \n",
    "    \n",
    "#     print('RMSE', file = f)\n",
    "#     print('loss info at level 2 (AHP)', file = f)\n",
    "#     print('    level 3', file = f)\n",
    "#     print('      ' + str(character_datasets_rmse['AHP_pred_df_std_loss_l2 bottom_level_cols'][2]), file = f)\n",
    "#     print('    level 2', file = f)\n",
    "#     print('      ' + str(character_datasets_rmse['AHP_pred_df_std_loss_l2 sec_level_cols'][2]), file = f)\n",
    "#     print('MASE', file = f)\n",
    "#     print('loss info at level 2 (AHP)', file = f)\n",
    "#     print('    level 3', file = f)\n",
    "#     print('      ' + str(character_datasets_mase['AHP_pred_df_std_loss_l2 bottom_level_cols'][2]), file = f)\n",
    "#     print('    level 2', file = f)\n",
    "#     print('      ' + str(character_datasets_mase['AHP_pred_df_std_loss_l2 sec_level_cols'][2]), file = f)\n",
    "    \n",
    "\n",
    "#     print('RMSE', file = f)\n",
    "#     print('loss info at level 2 (PHA)', file = f)\n",
    "#     print('    level 3', file = f)\n",
    "#     print('      ' + str(character_datasets_rmse['PHA_pred_df_std_loss_l2 bottom_level_cols'][2]), file = f)\n",
    "#     print('    level 2', file = f)\n",
    "#     print('      ' + str(character_datasets_rmse['AHP_pred_df_std_loss_l2 sec_level_cols'][2]), file = f)\n",
    "    \n",
    "\n",
    "#     print('MASE', file = f)\n",
    "#     print('loss info at level 2 (PHA)', file = f)\n",
    "#     print('    level 3', file = f)\n",
    "#     print('      ' + str(character_datasets_mase['PHA_pred_df_std_loss_l2 bottom_level_cols'][2]), file = f)\n",
    "#     print('    level 2', file = f)\n",
    "#     print('      ' + str(character_datasets_mase['PHA_pred_df_std_loss_l2 sec_level_cols'][2]), file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #result for sequential aggregation\n",
    "\n",
    "# file_path = os.getcwd() + '\\\\' + 'results' + '\\\\' + 'accuracy ' + rule_name\n",
    "\n",
    "# with open(file_path, 'w+') as f:\n",
    "#     print('RMSE', file = f)\n",
    "#     print('All info aval (most accurate)', file = f)\n",
    "#     print('    level 3', file = f)\n",
    "#     print('      ' + str(character_datasets_rmse['AHP_pred_df_seq_full_info bottom_level_cols'][2]), file = f)\n",
    "#     print('    level 2', file = f)\n",
    "#     print('      ' + str(character_datasets_rmse['AHP_pred_df_seq_full_info sec_level_cols'][2]), file = f)\n",
    "#     print('    level 1', file = f)\n",
    "#     print('      ' + str(character_datasets_rmse['AHP_pred_df_seq_full_info fst_level_cols'][2]), file = f)\n",
    "#     print('    total', file = f)\n",
    "#     print('      ' + str(character_datasets_rmse['AHP_pred_df_seq_full_info total'][2]), file = f)   \n",
    "#     print()\n",
    "    \n",
    "#     print('MASE', file = f)\n",
    "#     print('All info aval (most accurate)', file = f)\n",
    "#     print('    level 3', file = f)\n",
    "#     print('      ' + str(character_datasets_mase['AHP_pred_df_seq_full_info bottom_level_cols'][2]), file = f)\n",
    "#     print('    level 2', file = f)\n",
    "#     print('      ' + str(character_datasets_mase['AHP_pred_df_seq_full_info sec_level_cols'][2]), file = f)\n",
    "#     print('    level 1', file = f)\n",
    "#     print('      ' + str(character_datasets_mase['AHP_pred_df_seq_full_info fst_level_cols'][2]), file = f)\n",
    "#     print('    total', file = f)\n",
    "#     print('      ' + str(character_datasets_mase['AHP_pred_df_seq_full_info total'][2]), file = f)   \n",
    "#     print('\\n', file = f)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     print('RMSE', file = f)\n",
    "#     print('loss info at level 3 (AHP)', file = f)\n",
    "#     print('    level 3', file = f)\n",
    "#     print('      ' + str(character_datasets_rmse['AHP_pred_df_seq_loss_l3 bottom_level_cols'][2]), file = f)\n",
    "\n",
    "#     print('RMSE', file = f)\n",
    "#     print('loss info at level 3 (PHA)', file = f)\n",
    "#     print('    level 3', file = f)\n",
    "#     print('      ' + str(character_datasets_rmse['PHA_pred_df_seq_loss_l3 bottom_level_cols'][2]), file = f)\n",
    "    \n",
    "#     print('MASE', file = f)\n",
    "#     print('loss info at level 3 (AHP)', file = f)\n",
    "#     print('    level 3', file = f)\n",
    "#     print('      ' + str(character_datasets_mase['AHP_pred_df_seq_loss_l3 bottom_level_cols'][2]), file = f)\n",
    "\n",
    "#     print('MASE', file = f)\n",
    "#     print('loss info at level 3 (PHA)', file = f)\n",
    "#     print('    level 3', file = f)\n",
    "#     print('      ' + str(character_datasets_mase['PHA_pred_df_seq_loss_l3 bottom_level_cols'][2]), file = f)\n",
    "    \n",
    "#     print('\\n', file = f)\n",
    "\n",
    "    \n",
    "    \n",
    "#     print('RMSE', file = f)\n",
    "#     print('loss info at level 2 (AHP)', file = f)\n",
    "#     print('    level 3', file = f)\n",
    "#     print('      ' + str(character_datasets_rmse['AHP_pred_df_seq_loss_l2 bottom_level_cols'][2]), file = f)\n",
    "#     print('    level 2', file = f)\n",
    "#     print('      ' + str(character_datasets_rmse['AHP_pred_df_seq_loss_l2 sec_level_cols'][2]), file = f)\n",
    "\n",
    "#     print('MASE', file = f)\n",
    "#     print('loss info at level 2 (AHP)', file = f)\n",
    "#     print('    level 3', file = f)\n",
    "#     print('      ' + str(character_datasets_mase['AHP_pred_df_seq_loss_l2 bottom_level_cols'][2]), file = f)\n",
    "#     print('    level 2', file = f)\n",
    "#     print('      ' + str(character_datasets_mase['AHP_pred_df_seq_loss_l2 sec_level_cols'][2]), file = f)\n",
    "    \n",
    "\n",
    "#     print('RMSE', file = f)\n",
    "#     print('loss info at level 2 (PHA)', file = f)\n",
    "#     print('    level 3', file = f)\n",
    "#     print('      ' + str(character_datasets_rmse['PHA_pred_df_seq_loss_l2 bottom_level_cols'][2]), file = f)\n",
    "#     print('    level 2', file = f)\n",
    "#     print('      ' + str(character_datasets_rmse['AHP_pred_df_seq_loss_l2 sec_level_cols'][2]), file = f)\n",
    "    \n",
    "#     print('MASE', file = f)\n",
    "#     print('loss info at level 2 (PHA)', file = f)\n",
    "#     print('    level 3', file = f)\n",
    "#     print('      ' + str(character_datasets_mase['PHA_pred_df_seq_loss_l2 bottom_level_cols'][2]), file = f)\n",
    "#     print('    level 2', file = f)\n",
    "#     print('      ' + str(character_datasets_mase['PHA_pred_df_seq_loss_l2 sec_level_cols'][2]), file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################### result for standard aggregation\n",
    "file_path = os.getcwd() + '\\\\' + 'results' + '\\\\' + 'fairness_standard_fixed ' + rule_name\n",
    "with open(file_path, 'w+') as f:\n",
    "    print('fairness_actual, pred, diff', file = f)\n",
    "    print('All info aval (most accurate)', file = f)\n",
    "    print('    level 3', file = f)\n",
    "    print('      ' + str(character_datasets_fairness['PHA_pred_df_std_full_info bottom_level_cols'][0]), file = f)\n",
    "    print('      ' + str(character_datasets_fairness['PHA_pred_df_std_full_info bottom_level_cols'][1]), file = f)\n",
    "    print('      ' + str(character_datasets_fairness['PHA_pred_df_std_full_info bottom_level_cols'][1] - character_datasets_fairness['PHA_pred_df_std_full_info bottom_level_cols'][0]), file = f)\n",
    "    \n",
    "    \n",
    "    print('    level 2', file = f)\n",
    "    print('      ' + str(character_datasets_fairness['PHA_pred_df_std_full_info sec_level_cols'][0]), file = f)\n",
    "    print('      ' + str(character_datasets_fairness['PHA_pred_df_std_full_info sec_level_cols'][1]), file = f)\n",
    "    print('      ' + str(character_datasets_fairness['PHA_pred_df_std_full_info sec_level_cols'][1] - character_datasets_fairness['PHA_pred_df_std_full_info sec_level_cols'][0]), file = f)\n",
    "\n",
    "    print('    level 1', file = f)\n",
    "    print('      ' + str(character_datasets_fairness['PHA_pred_df_std_full_info fst_level_cols'][0]), file = f)\n",
    "    print('      ' + str(character_datasets_fairness['PHA_pred_df_std_full_info fst_level_cols'][1]), file = f)\n",
    "    print('      ' + str(character_datasets_fairness['PHA_pred_df_std_full_info fst_level_cols'][1] - character_datasets_fairness['PHA_pred_df_std_full_info fst_level_cols'][0]), file = f)\n",
    "\n",
    "\n",
    "    print('\\n', file = f)\n",
    "    \n",
    "\n",
    "    print('\\n', file = f)\n",
    "    \n",
    "\n",
    "\n",
    "    print('fairness_acutal, pred, diff', file = f)\n",
    "    print('loss info at level 3', file = f)\n",
    "    print('    level 3', file = f)\n",
    "    print('      ' + str(character_datasets_fairness['PHA_pred_df_std_full_info bottom_level_cols'][0]), file = f)\n",
    "\n",
    "    print('      ' + str(character_datasets_fairness['PHA_pred_df_std_loss_l3 bottom_level_cols'][1]), file = f)\n",
    "    print('      ' + str(character_datasets_fairness['PHA_pred_df_std_loss_l3 bottom_level_cols'][1] - character_datasets_fairness['PHA_pred_df_std_full_info bottom_level_cols'][0]), file = f)\n",
    "\n",
    "    print('\\n', file = f)\n",
    "    \n",
    "\n",
    "    print('\\n', file = f)\n",
    "    \n",
    "    print('fairness_actual, pred, diff ', file = f)\n",
    "    print('loss info at level 2', file = f)\n",
    "    print('    level 3', file = f)\n",
    "    print('      ' + str(character_datasets_fairness['PHA_pred_df_std_full_info bottom_level_cols'][0]), file = f)\n",
    "    print('      ' + str(character_datasets_fairness['PHA_pred_df_std_loss_l2 bottom_level_cols'][1]), file = f)\n",
    "    print('      ' + str(character_datasets_fairness['PHA_pred_df_std_loss_l2 bottom_level_cols'][1] - character_datasets_fairness['PHA_pred_df_std_full_info bottom_level_cols'][0]), file = f)\n",
    "\n",
    "    print('    level 2', file = f)\n",
    "    print('      ' + str(character_datasets_fairness['PHA_pred_df_std_full_info sec_level_cols'][0]), file = f)\n",
    "    print('      ' + str(character_datasets_fairness['PHA_pred_df_std_loss_l2 sec_level_cols'][1]), file = f)\n",
    "    print('      ' + str(character_datasets_fairness['PHA_pred_df_std_loss_l2 sec_level_cols'][1] - character_datasets_fairness['PHA_pred_df_std_full_info sec_level_cols'][0]), file = f)\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################### result for sequential aggregation\n",
    "file_path = os.getcwd() + '\\\\' + 'results' + '\\\\' + 'fairness_fixed ' + rule_name\n",
    "with open(file_path, 'w+') as f:\n",
    "    print('fairness_actual, pred, diff', file = f)\n",
    "    print('All info aval (most accurate)', file = f)\n",
    "    print('    level 3', file = f)\n",
    "    print('      ' + str(character_datasets_fairness['PHA_pred_df_seq_full_info bottom_level_cols'][0]), file = f)\n",
    "    print('      ' + str(character_datasets_fairness['PHA_pred_df_seq_full_info bottom_level_cols'][1]), file = f)\n",
    "    print('      ' + str(character_datasets_fairness['PHA_pred_df_seq_full_info bottom_level_cols'][1] - character_datasets_fairness['PHA_pred_df_seq_full_info bottom_level_cols'][0]), file = f)\n",
    "    \n",
    "    \n",
    "    print('    level 2', file = f)\n",
    "    print('      ' + str(character_datasets_fairness['PHA_pred_df_seq_full_info sec_level_cols'][0]), file = f)\n",
    "    print('      ' + str(character_datasets_fairness['PHA_pred_df_seq_full_info sec_level_cols'][1]), file = f)\n",
    "    print('      ' + str(character_datasets_fairness['PHA_pred_df_seq_full_info sec_level_cols'][1] - character_datasets_fairness['PHA_pred_df_seq_full_info sec_level_cols'][0]), file = f)\n",
    "\n",
    "    print('    level 1', file = f)\n",
    "    print('      ' + str(character_datasets_fairness['PHA_pred_df_seq_full_info fst_level_cols'][0]), file = f)\n",
    "    print('      ' + str(character_datasets_fairness['PHA_pred_df_seq_full_info fst_level_cols'][1]), file = f)\n",
    "    print('      ' + str(character_datasets_fairness['PHA_pred_df_seq_full_info fst_level_cols'][1] - character_datasets_fairness['PHA_pred_df_seq_full_info fst_level_cols'][0]), file = f)\n",
    "\n",
    "\n",
    "    print('\\n', file = f)\n",
    "    \n",
    "\n",
    "    print('\\n', file = f)\n",
    "    \n",
    "\n",
    "\n",
    "    print('fairness_acutal, pred, diff', file = f)\n",
    "    print('loss info at level 3', file = f)\n",
    "    print('    level 3', file = f)\n",
    "    print('      ' + str(character_datasets_fairness['PHA_pred_df_seq_full_info bottom_level_cols'][0]), file = f)\n",
    "\n",
    "    print('      ' + str(character_datasets_fairness['PHA_pred_df_seq_loss_l3 bottom_level_cols'][1]), file = f)\n",
    "    print('      ' + str(character_datasets_fairness['PHA_pred_df_seq_loss_l3 bottom_level_cols'][1] - character_datasets_fairness['PHA_pred_df_seq_full_info bottom_level_cols'][0]), file = f)\n",
    "\n",
    "    print('\\n', file = f)\n",
    "    \n",
    "\n",
    "    print('\\n', file = f)\n",
    "    \n",
    "    print('fairness_actual, pred, diff ', file = f)\n",
    "    print('loss info at level 2', file = f)\n",
    "    print('    level 3', file = f)\n",
    "    print('      ' + str(character_datasets_fairness['PHA_pred_df_seq_full_info bottom_level_cols'][0]), file = f)\n",
    "    print('      ' + str(character_datasets_fairness['PHA_pred_df_seq_loss_l2 bottom_level_cols'][1]), file = f)\n",
    "    print('      ' + str(character_datasets_fairness['PHA_pred_df_seq_loss_l2 bottom_level_cols'][1] - character_datasets_fairness['PHA_pred_df_seq_full_info bottom_level_cols'][0]), file = f)\n",
    "\n",
    "    print('    level 2', file = f)\n",
    "    print('      ' + str(character_datasets_fairness['PHA_pred_df_seq_full_info sec_level_cols'][0]), file = f)\n",
    "    print('      ' + str(character_datasets_fairness['PHA_pred_df_seq_loss_l2 sec_level_cols'][1]), file = f)\n",
    "    print('      ' + str(character_datasets_fairness['PHA_pred_df_seq_loss_l2 sec_level_cols'][1] - character_datasets_fairness['PHA_pred_df_seq_full_info sec_level_cols'][0]), file = f)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# directory = os.getcwd() + '\\\\' + 'data'\n",
    "# files = os.listdir(directory)\n",
    "# dfs = {}\n",
    "# dfs_standard = {}\n",
    "# dfs_sequential = {}\n",
    "# for filename in files: # read all the files\n",
    "#     if filename.endswith('.csv'):\n",
    "#         #concast the file name\n",
    "#         dir_filename = \"data/\"+filename\n",
    "#         f = pd.read_csv(dir_filename, sep=\"\\t\" , encoding=\"utf-16-le\",  thousands=',')        \n",
    "#         dfs[filename] = f\n",
    "\n",
    "\n",
    "# for name in dfs:\n",
    "\n",
    "#     # get the features of df\n",
    "#     df_name = name\n",
    "#     df_name = df_name.split('.')[0].split('_')\n",
    "#     Student_level = df_name[0]\n",
    "#     First_gen_status = df_name[1]\n",
    "#     Enrollment_status = df_name[2]\n",
    "#     Gender = df_name[3]\n",
    "#     Residency = df_name[4]\n",
    "#     Entry_status = df_name[5]\n",
    "    \n",
    "    \n",
    "#     # operation for datasets\n",
    "#     df = dfs[name]\n",
    "#     df = df.drop(df.index[-1])\n",
    "#     data_2010_columns = [\"Category\", '2010', '2011', '2012', '2013','2014', '2015','2016', '2017', '2018', '2019']\n",
    "#     df = df[data_2010_columns]\n",
    "#     df = df.groupby(\"Category\").sum()\n",
    "   \n",
    "#     # transpose\n",
    "#     df = df.T\n",
    "#     df = df.rename_axis(None, axis = 1).rename_axis('year', axis = 0)\n",
    "#     df = df.reset_index()\n",
    "#     # store the dataframes into df_standard    \n",
    "\n",
    "#     # melt the dataset\n",
    "#     columns = df.columns # get all the columns' names\n",
    "#     columns = columns[1:] # get the columns names except for Year\n",
    "\n",
    "#     df = pd.melt(df, id_vars=[\"year\"], value_vars=columns)\n",
    "#     df = df.rename(columns = {0:'Race', 'value':'count'})\n",
    "\n",
    "#     # add columns for other features\n",
    "#     df[\"Student_Level\"] = Student_level\n",
    "#     df[\"First_gen_status\"] = First_gen_status\n",
    "#     df[\"Enrollment_status\"] = Enrollment_status\n",
    "#     df[\"Gender\"] = Gender\n",
    "#     df[\"Residency\"] = Residency\n",
    "#     df[\"Entry_status\"] = Entry_status\n",
    "    \n",
    "#     dfs_standard[name] = df\n",
    "\n",
    "# ##################### merge the datasets ########################################\n",
    "\n",
    "# names = list(dfs_standard.keys())\n",
    "# datasets = list(dfs_standard.values())\n",
    "# # concat all the datasets together\n",
    "# df_standard = pd.concat(datasets, sort=False, ignore_index = True)\n",
    "# df_standard = df_standard.sort_values(by = ['year','Student_Level','Residency','Enrollment_status','Gender'])\n",
    "# df_standard = df_standard.reset_index().drop(['index'], axis = 1)\n",
    "# df_standard = df_standard.rename(columns = {'variable':'race'})\n",
    "# df_standard.columns= df_standard.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df_standard\n",
    "\n",
    "# ## 1 level: total -> first_gen_status\n",
    "# df_std_l3 = df.pivot_table(index= \"year\", columns = [\"race\", \"gender\", \"first_gen_status\"], values = \"count\", aggfunc= \"sum\")\n",
    "# df_std_l3.columns = ['_'.join(str(s).strip() for s in col if s) for col in df_std_l3.columns]\n",
    "\n",
    "# ## 2 levels: total -> race -> gender\n",
    "# df_std_l2 = df.pivot_table(index= \"year\", columns = [\"race\", \"gender\"], values = \"count\", aggfunc= \"sum\")\n",
    "# df_std_l2.columns = ['_'.join(str(s).strip() for s in col if s) for col in df_std_l2.columns]\n",
    "\n",
    "# ## 1 level: total -> first_gen_status\n",
    "# df_std_l1 = df.pivot_table(index= \"year\", columns = [\"race\"], values = \"count\", aggfunc= \"sum\")\n",
    "\n",
    "# ## total\n",
    "# df_total = df.pivot_table(index= \"year\", values = \"count\", aggfunc= \"sum\")\n",
    "# df_total = df_total.rename(columns={\"count\": \"total\"})\n",
    "\n",
    "# # join the dateframes\n",
    "# df_std_full_info = df_total.join(df_std_l1).join(df_std_l2).join(df_std_l3)\n",
    "\n",
    "# # change the index year to day-time format\n",
    "# df_std_full_info.index = pd.to_datetime(df_std_full_info.index, format='%Y')\n",
    "# df_std_full_info = df_std_full_info.to_period(\"Y\")\n",
    "# df_std_full_info = df_std_full_info.to_timestamp()\n",
    "# df_std_full_info\n",
    "# df_std_full_info.head(10)\n",
    "# df_std_full_info['Azerbaijani_fem_first'] = 0\n",
    "\n",
    "# df_std_full_info['Bahraini_male_first'] = 0\n",
    "# 'Djiboutian_male_first'\n",
    "# df_std_full_info['Djiboutian_male_first'] = 0\n",
    "# df_std_full_info['Djiboutian_fem_first'] = 0\n",
    "# df_std_full_info['Djiboutian_male_nFirst'] = 0\n",
    "# df_std_full_info['Emerati_male_first'] = 0\n",
    "\n",
    "# df_std_full_info['Emerati_male_first'] = 0\n",
    "# df_std_full_info['Georgian_male_first'] = 0\n",
    "# df_std_full_info['Libyan_fem_first'] = 0\n",
    "# df_std_full_info['Mauritanian_male_first'] = 0\n",
    "# df_std_full_info['Mauritanian_male_nFirst'] = 0\n",
    "# df_std_full_info['Native Hawaiian and Pacific Islander_male_first'] = 0\n",
    "# df_std_full_info['Native Hawaiian and Pacific Islander_fem_first'] = 0\n",
    "# df_std_full_info['Native Hawaiian and Pacific Islander_male_nFirst'] = 0\n",
    "# df_std_full_info['Omani_fem_first'] = 0\n",
    "# df_std_full_info['Omani_fem_nFirst'] = 0\n",
    "# df_std_full_info['Other North African_fem_first'] = 0\n",
    "# df_std_full_info['Qatari_male_first'] = 0\n",
    "# df_std_full_info['Omani_fem_first'] = 0\n",
    "# df_std_full_info['Qatari_fem_first'] = 0\n",
    "# df_std_full_info['Qatari_fem_nFirst'] = 0\n",
    "# df_std_full_info['Tunisian_fem_first'] = 0\n",
    "# df_std_full_info['Omani_fem'] = 0\n",
    "# df_std_full_info['Qatari_fem'] = 0\n",
    "# df_std_full_info['Tunisian_fem_first'] = 0\n",
    "# df_std_full_info['Tunisian_fem_first'] = 0\n",
    "# df_std_full_info['Tunisian_fem_first'] = 0\n",
    "\n",
    "\n",
    "\n",
    "# character_datasets = {\n",
    "    \n",
    "    \n",
    "#             'AHP_pred_df_std_full_info':df_std_full_info\n",
    "\n",
    "#             }\n",
    "\n",
    "# ##################### To test what epsilon to use \n",
    "# for x in character_datasets:\n",
    "#     diffs = {}\n",
    "#     # bottom level\n",
    "#     bottom_level_cols = [col for col in character_datasets[x].columns if col.count(\"_\") == 2]\n",
    "#     # second level\n",
    "\n",
    "#     cols = bottom_level_cols\n",
    "#     levels = [\"l3\"]\n",
    "#     for i in range(0, len(levels)):\n",
    "#         # get the mean of all the time periods \n",
    "#         # drop the total \n",
    "#         df_actual = pd.DataFrame(character_datasets[x].mean(axis = 0)).T\n",
    "\n",
    "#         level = levels[i]\n",
    "    \n",
    "#         totalfair_actual, totalfair_pred, diff = total_fairness(df_actual,df_actual,0.02,level)\n",
    "#         diffs[level] = diff\n",
    "\n",
    "       \n",
    "\n",
    "# plt.figure(figsize=(10, 8), dpi=80)\n",
    "\n",
    "# plt.plot(diffs['l3'], color='pink', marker='.', ls = '') #plot the data\n",
    "\n",
    "# X = [0, 40, 5500]\n",
    "# Y1 = [0.02, 0.02, 0.02]\n",
    "# Y2 = [0.10, 0.10, 0.10]\n",
    "# Y3= [0.20, 0.20, 0.20]\n",
    "# Y4= [0.40, 0.40, 0.40]\n",
    "# plt.axis('off')\n",
    "# plt.plot(X,Y1, label = 'epsilon = 0.01')\n",
    "# plt.plot(X,Y2, label = 'epsilon = 0.05')\n",
    "# plt.plot(X,Y3, label = 'epsilon = 0.10')\n",
    "# plt.plot(X,Y4, label = 'epsilon = 0.20')\n",
    "# plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###################### To test what epsilon to use \n",
    "# character_actual_datasets = {\n",
    "    \n",
    "    \n",
    "#             'AHP_pred_df_std_full_info':AHP_pred_df_std_full_info,\n",
    "\n",
    "#             }\n",
    "# for x in character_actual_datasets:\n",
    "#     diffs = []\n",
    "#     # bottom level\n",
    "#     bottom_level_cols = [col for col in character_datasets[x].columns if col.count(\"_\") == 2]\n",
    "#     # second level\n",
    "#     sec_level_cols = [col for col in character_datasets[x].columns if col.count(\"_\") == 1]\n",
    "#     # first level\n",
    "#     fst_level_cols = [col for col in character_datasets[x].columns if (col.count(\"_\") == 0 and col != \"total\")]\n",
    "#     # cols\n",
    "#     cols = bottom_level_cols + sec_level_cols + fst_level_cols\n",
    "#     levels = [\"l3\", 'l2', 'l1']\n",
    "#     for i in range(0, len(levels)):\n",
    "#         # get the mean of all the time periods \n",
    "#         # drop the total \n",
    "#         df_actual = pd.DataFrame(character_datasets[x].mean(axis = 0)).T\n",
    "#         df_pred = character_datasets[x].iloc[-1:]\n",
    "        \n",
    "#         level = levels[i]\n",
    "#         totalfair_actual, totalfair_pred, diff = total_fairness(df_actual,df_pred,0.02,level)\n",
    "#         diffs.append(diff)\n",
    "#         key = x + \" \" + levels_name[i]\n",
    "#         character_datasets_fairness[key] = [totalfair_actual, totalfair_pred]\n",
    "\n",
    "# plt.plot(diffs[0], color='magenta', marker='D', ls = '', label = 'level 3') #plot the data\n",
    "# plt.plot(diffs[1], color='magenta', marker='D', ls = '', label = 'level 2') #plot the data\n",
    "# plt.plot(diffs[2], color='magenta', marker='D', ls = '', label = 'level 1')\n",
    "# plt.plot(diffs[2], color='magenta', marker='D', ls = '', label = 'level 1')\n",
    "# X = [0, 40, 75]\n",
    "# Y1 = [0.02, 0.02, 0.02]\n",
    "# Y2 = [0.05, 0.05, 0.05]\n",
    "# Y3= [0.10, 0.10, 0.10]\n",
    "# Y4= [0.20, 0.20, 0.20]\n",
    "\n",
    "# plt.plot(X,Y1)\n",
    "# plt.plot(X,Y2)\n",
    "# plt.plot(X,Y3)\n",
    "# plt.plot(X,Y4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l3_x_acc = [0.76, 0.72, 0.73, 0.73, 0.79, 0.79, 0.81, 0.76, 0.88, 0.89, 0.82, 0.79, 0.83, 0.70, 0.76, 0.82, 0.99, 0.83, 0.77, 0.78, 0.73]\n",
    "# l3_y_fair_acc = [0.009, 0.176 , 0.055, 0.139, 0.000, 0.018, 0.028, 0.000, 0.315, 0.583, 0.315, 0.352, 0.361, 0.000, 0.185, 0.213, 0.537, 0.019, 0.593, 0.055, 0.055]\n",
    "# l3_y_fairness = [0.537, 0.352, 0.630, 0.676, 0.565, 0.852, 0.324, 0.324 ,0.278, 0.889, 0.639, 0.861, 0.639, 0.537, 0.500, 0.315, 0.814, 0.843, 0.333, 0.750 ,0.028]\n",
    "# plt.scatter(l3_x_acc,l3_y_fairness, label = 'Level 3')\n",
    "\n",
    "# l2_x_acc = [0.76, 0.64, 0.59, 0.67, 0.73 , 0.72, 0.70, 0.70, 0.79, 0.86, 0.65, 0.85, 0.82, 0.78, 0.54, 0.79, 0.85, 0.70, 0.61, 0.59, 0.58]\n",
    "# l2_y_fairness = [0.222, 0.194 , 0.139, 0.139, 0.111, 0.194, 0.167, 0.194, 0.167, 0.139, 0.278, 0.139, 0.139, 0.139, 0.750, 0.167 ,0.167, 0.111, 0.222, 0.139, 0.139]\n",
    "# l2_y_fair_acc = [0.000, 0.111, 0.028, 0.083, 0.083, 0.111, 0.056, 0.055, 0.000, 0.028, 0.111, 0.028, 0.083, 0.083, 0.083, 0.084, 0.000, 0.028, 0.056, 0.056, 0.028]\n",
    "# plt.scatter(l2_x_acc,l2_y_fairness, label = 'Level 2')\n",
    "\n",
    "\n",
    "# l1_x_acc = [0.64, 0.49, 0.57, 0.65, 0.67, 0.61, 0.63, 0.52, 0.77, 0.75, 0.60, 0.77, 0.80, 0.65, 0.39, 0.74, 0.79, 0.63, 0.49, 0.52, 0.50]\n",
    "# l1_y_fairness = [0.250,0.139, 0.111, 0.139, 0.083, 0.111, 0.056, 0.083, 0.167, 0.111, 0.056, 0.111, 0.056, 0.056, 0.333, 0.222, 0.083, 0.167, 0.222, 0.083, 0.028]\n",
    "# l1_y_fair_acc = [0.083, 0.000, 0.000, 0.056, 0.027, 0.000, 0.028, 0.000, 0.056, 0.000, 0.028, 0.000, 0.027, 0.000, 0.028, 0.028, 0.056, 0.000, 0.055, 0.028, 0.055]\n",
    "\n",
    "# plt.scatter(l1_x_acc,l1_y_fairness, label = 'Level 1')\n",
    "\n",
    "# plt.xlabel(\"MASE\")\n",
    "# plt.ylabel(\"Total fairness\")\n",
    "# plt.legend(prop={'size': 15})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(l3_x_acc,l3_y_fair_acc, label = 'Level 3')\n",
    "# plt.scatter(l2_x_acc,l2_y_fair_acc, label = 'Level 2')\n",
    "# plt.scatter(l1_x_acc,l1_y_fair_acc, label = 'Level 1')\n",
    "# plt.xlabel(\"Firness \")\n",
    "# plt.ylabel(\"Total fairness\")\n",
    "# plt.legend(prop={'size': 15})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 ('fullcombo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a2a736f2b245f4d7bc218b80048c82ceb23b183a7b82259fe09748f43e77e76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
